{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53cd153d",
   "metadata": {},
   "source": [
    "# Proceso ETL: Una Gu√≠a Pr√°ctica con Python\n",
    "\n",
    "## üìö Objetivos de Aprendizaje\n",
    "\n",
    "Al completar este notebook, los estudiantes ser√°n capaces de:\n",
    "- Comprender los conceptos fundamentales de ETL (Extract, Transform, Load)\n",
    "- Implementar cada fase del proceso ETL usando Python y pandas\n",
    "- Aplicar t√©cnicas de limpieza y transformaci√≥n de datos\n",
    "- Manejar errores comunes en pipelines de datos\n",
    "- Dise√±ar pipelines ETL escalables y mantenibles\n",
    "\n",
    "## üéØ ¬øQu√© es ETL?\n",
    "\n",
    "**ETL** es un proceso fundamental en ingenier√≠a de datos que permite:\n",
    "- **Extract (Extraer)**: Obtener datos de m√∫ltiples fuentes\n",
    "- **Transform (Transformar)**: Limpiar, validar y estructurar los datos\n",
    "- **Load (Cargar)**: Almacenar los datos procesados en el destino final\n",
    "\n",
    "Este notebook es una gu√≠a completa que cubre desde conceptos b√°sicos hasta temas avanzados, con ejemplos pr√°cticos y ejercicios para trabajar con pipelines ETL en entornos reales de ingenier√≠a de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec1edb",
   "metadata": {},
   "source": [
    "## El Pipeline ETL de Referencia\n",
    "\n",
    "Nos basaremos en la siguiente estructura de pipeline, que representa un flujo ETL cl√°sico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_pipeline():\n",
    "    # 1. Extraer datos desde una fuente original\n",
    "    raw_data = extract_from_database()\n",
    "    \n",
    "    # 2. Transformar los datos aplicando reglas y limpiando\n",
    "    cleaned_data = apply_business_rules(raw_data)\n",
    "    \n",
    "    # 3. Normalizar el esquema para que sea consistente\n",
    "    structured_data = normalize_schema(cleaned_data)\n",
    "    \n",
    "    # 4. Cargar los datos transformados a su destino final\n",
    "    load_to_warehouse(structured_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b297d4",
   "metadata": {},
   "source": [
    "A continuaci√≥n, implementaremos cada una de estas funciones con ejemplos claros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e9b2f4",
   "metadata": {},
   "source": [
    "## üì¶ Librer√≠as y Configuraci√≥n Inicial\n",
    "\n",
    "Para nuestros ejemplos, usaremos las siguientes librer√≠as esenciales en ingenier√≠a de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94c3457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd          # Manipulaci√≥n y an√°lisis de datos\n",
    "import sqlite3              # Base de datos SQL ligera\n",
    "import numpy as np          # Operaciones num√©ricas\n",
    "import json                 # Manejo de datos JSON\n",
    "import logging              # Sistema de logs\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de logging para monitorear el pipeline\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c7036",
   "metadata": {},
   "source": [
    "## üß† Conceptos Fundamentales de ETL\n",
    "\n",
    "### Tipos de Procesamiento ETL\n",
    "\n",
    "1. **Batch Processing (Procesamiento por lotes)**\n",
    "   - Procesa grandes vol√∫menes de datos en intervalos programados\n",
    "   - Ideal para reportes diarios, semanales o mensuales\n",
    "   - Ejemplo: Procesar todas las transacciones del d√≠a anterior\n",
    "\n",
    "2. **Real-time Processing (Procesamiento en tiempo real)**\n",
    "   - Procesa datos tan pronto como llegan\n",
    "   - Cr√≠tico para aplicaciones que requieren respuesta inmediata\n",
    "   - Ejemplo: Detecci√≥n de fraude en transacciones\n",
    "\n",
    "3. **Near Real-time Processing (Procesamiento casi en tiempo real)**\n",
    "   - Procesa datos con un peque√±o retraso (segundos o minutos)\n",
    "   - Balance entre velocidad y eficiencia de recursos\n",
    "   - Ejemplo: Actualizaci√≥n de dashboards cada 5 minutos\n",
    "\n",
    "### Calidad de Datos - Las 6 Dimensiones\n",
    "\n",
    "1. **Exactitud**: Los datos reflejan la realidad\n",
    "2. **Completitud**: No hay valores faltantes cr√≠ticos\n",
    "3. **Consistencia**: Los datos son coherentes entre sistemas\n",
    "4. **Validez**: Los datos cumplen con reglas de negocio\n",
    "5. **Unicidad**: No hay duplicados no deseados\n",
    "6. **Actualidad**: Los datos est√°n actualizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae53585",
   "metadata": {},
   "source": [
    "## 1. Extract: Extracci√≥n de Datos\n",
    "\n",
    "### üéØ Objetivos de la Fase Extract\n",
    "- Conectar con m√∫ltiples fuentes de datos\n",
    "- Extraer datos de manera eficiente\n",
    "- Manejar diferentes formatos y protocolos\n",
    "- Implementar estrategias de extracci√≥n incremental\n",
    "\n",
    "### üìä Fuentes de Datos Comunes\n",
    "- **Bases de datos relacionales**: MySQL, PostgreSQL, SQL Server\n",
    "- **Bases de datos NoSQL**: MongoDB, Cassandra, Redis\n",
    "- **Archivos**: CSV, JSON, XML, Parquet\n",
    "- **APIs REST**: Servicios web y microservicios\n",
    "- **Sistemas de streaming**: Kafka, Kinesis\n",
    "\n",
    "**Ejemplo:** Vamos a simular la extracci√≥n de datos de una base de datos SQL. Crearemos una peque√±a base de datos en memoria con `sqlite3` y luego la leeremos usando `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd18618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1bcdc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_database():\n",
    "    \"\"\"\n",
    "    Simula la extracci√≥n de datos desde una base de datos.\n",
    "    En un entorno real, esto se conectar√≠a a una base de datos real.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Datos crudos extra√≠dos\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Iniciando extracci√≥n de datos...\")\n",
    "        \n",
    "        # Simulamos datos con problemas t√≠picos de calidad\n",
    "        data = {\n",
    "            'ID_USER': [101, 102, 103, 104, 105, 106],\n",
    "            'user_name': ['Ana', 'Luis', 'Marta', 'Juan', 'Eva', None],  # Valor nulo\n",
    "            'registration_date': ['2025-01-15', '2025-02-20', '2025-03-01', '2025-04-10', '2025-05-19', '2025-06-30'],\n",
    "            'total_spent': [150.5, 80.0, -999, 200.0, 45.25, 'invalid'],  # Valor inv√°lido\n",
    "            'country_code': ['ES', 'MX', 'ES', 'CO', 'es', 'ES'],  # Inconsistencia en may√∫sculas\n",
    "            'email': ['ana@email.com', 'luis@email.com', 'marta@email.com', 'juan@email.com', 'eva@email.com', 'pedro@email.com']\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        print(\"--- 1. Datos Crudos Extra√≠dos ---\")\n",
    "        print(f\"Registros extra√≠dos: {len(df)}\")\n",
    "        print(f\"Columnas: {list(df.columns)}\")\n",
    "        print(\"\\nPrimeras filas:\")\n",
    "        print(df)\n",
    "        print(\"\\nInformaci√≥n del DataFrame:\")\n",
    "        print(df.info())\n",
    "        print(\"\\n geometria del DataFrame:\")\n",
    "        print(df.shape)\n",
    "        print(\"\\n Estad√≠sticas descriptivas:\")\n",
    "        print (df.describe())\n",
    "        print('')\n",
    "        \n",
    "        logger.info(f\"Extracci√≥n completada: {len(df)} registros\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la extracci√≥n: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89677c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d2bafe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6 entries, 0 to 5\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   ID_USER            6 non-null      int64 \n",
      " 1   user_name          5 non-null      object\n",
      " 2   registration_date  6 non-null      object\n",
      " 3   total_spent        6 non-null      object\n",
      " 4   country_code       6 non-null      object\n",
      " 5   email              6 non-null      object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 420.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d280cc5e",
   "metadata": {},
   "source": [
    "### ‚ñ∂Ô∏è Ejecutemos la Extracci√≥n\n",
    "\n",
    "Ahora vamos a ejecutar la funci√≥n de extracci√≥n para ver los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f50a460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 19:30:15,671 - INFO - Iniciando extracci√≥n de datos...\n",
      "2025-11-06 19:30:15,682 - INFO - Extracci√≥n completada: 6 registros\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Datos Crudos Extra√≠dos ---\n",
      "Registros extra√≠dos: 6\n",
      "Columnas: ['ID_USER', 'user_name', 'registration_date', 'total_spent', 'country_code', 'email']\n",
      "\n",
      "Primeras filas:\n",
      "   ID_USER user_name registration_date total_spent country_code  \\\n",
      "0      101       Ana        2025-01-15       150.5           ES   \n",
      "1      102      Luis        2025-02-20        80.0           MX   \n",
      "2      103     Marta        2025-03-01        -999           ES   \n",
      "3      104      Juan        2025-04-10       200.0           CO   \n",
      "4      105       Eva        2025-05-19       45.25           es   \n",
      "5      106      None        2025-06-30     invalid           ES   \n",
      "\n",
      "             email  \n",
      "0    ana@email.com  \n",
      "1   luis@email.com  \n",
      "2  marta@email.com  \n",
      "3   juan@email.com  \n",
      "4    eva@email.com  \n",
      "5  pedro@email.com  \n",
      "\n",
      "Informaci√≥n del DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6 entries, 0 to 5\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   ID_USER            6 non-null      int64 \n",
      " 1   user_name          5 non-null      object\n",
      " 2   registration_date  6 non-null      object\n",
      " 3   total_spent        6 non-null      object\n",
      " 4   country_code       6 non-null      object\n",
      " 5   email              6 non-null      object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 420.0+ bytes\n",
      "None\n",
      "\n",
      " geometria del DataFrame:\n",
      "(6, 6)\n",
      "\n",
      " Estad√≠sticas descriptivas:\n",
      "          ID_USER\n",
      "count    6.000000\n",
      "mean   103.500000\n",
      "std      1.870829\n",
      "min    101.000000\n",
      "25%    102.250000\n",
      "50%    103.500000\n",
      "75%    104.750000\n",
      "max    106.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la funci√≥n de extracci√≥n\n",
    "raw_data = extract_from_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac2fd69",
   "metadata": {},
   "source": [
    "### üìÅ Ejemplo: Extracci√≥n desde Archivo CSV\n",
    "\n",
    "Veamos c√≥mo extraer datos desde un archivo CSV (otra fuente com√∫n):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbfca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv(file_path='sample_products.csv'):\n",
    "    \"\"\"\n",
    "    Extrae datos desde un archivo CSV.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo CSV\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Datos extra√≠dos del CSV\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Crear un archivo CSV de ejemplo\n",
    "        sample_data = {\n",
    "            'product_id': [1, 2, 3, 4, 5],\n",
    "            'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones'],\n",
    "            'price': [999.99, 25.50, 75.00, 299.99, 89.99],\n",
    "            'category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Accessories']\n",
    "        }\n",
    "        \n",
    "        df_sample = pd.DataFrame(sample_data)\n",
    "        df_sample.to_csv(file_path, index=False)\n",
    "        \n",
    "        # Leer el archivo CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        print(\"--- Extracci√≥n desde CSV ---\")\n",
    "        print(df)\n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Archivo no encontrado: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error leyendo CSV: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Ejemplo de uso\n",
    "products_df = extract_from_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde5da4",
   "metadata": {},
   "source": [
    "## 2. Transform: Aplicaci√≥n de Reglas de Negocio\n",
    "\n",
    "### üéØ Objetivos de la Fase Transform\n",
    "- Limpiar datos inconsistentes o err√≥neos\n",
    "- Validar que los datos cumplan reglas de negocio\n",
    "- Enriquecer datos con informaci√≥n adicional\n",
    "- Aplicar transformaciones matem√°ticas o l√≥gicas\n",
    "- Normalizar formatos y estructuras\n",
    "\n",
    "### üîß T√©cnicas Comunes de Transformaci√≥n\n",
    "- **Limpieza**: Eliminar duplicados, corregir errores tipogr√°ficos\n",
    "- **Validaci√≥n**: Verificar rangos, formatos, tipos de datos\n",
    "- **Enriquecimiento**: Agregar datos calculados o de referencia\n",
    "- **Normalizaci√≥n**: Estandarizar formatos y escalas\n",
    "- **Agregaci√≥n**: Resumir datos por grupos o per√≠odos\n",
    "\n",
    "**Ejemplo:** Aplicaremos las siguientes reglas:\n",
    "1. Corregir valores err√≥neos: El valor `-999` en `total_spent` debe ser 0\n",
    "2. Corregir inconsistencias: El c√≥digo de pa√≠s `es` debe ser `ES`\n",
    "3. Filtrar datos: Solo usuarios con gasto mayor a 50\n",
    "4. Manejar valores nulos en nombres de usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ebf8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_business_rules(raw_data):\n",
    "    \"\"\"\n",
    "    Aplica reglas de negocio y limpieza de datos.\n",
    "    \n",
    "    Args:\n",
    "        raw_data (pd.DataFrame): Datos crudos a transformar\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Datos limpios y transformados\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Iniciando transformaci√≥n de datos...\")\n",
    "        cleaned_data = raw_data.copy()\n",
    "        \n",
    "        print(\"--- An√°lisis de Calidad de Datos ---\")\n",
    "        print(f\"Registros iniciales: {len(cleaned_data)}\")\n",
    "        print(f\"Valores nulos por columna:\")\n",
    "        print(cleaned_data.isnull().sum())\n",
    "        print()\n",
    "        \n",
    "        # 1. Manejar valores nulos en user_name\n",
    "        null_names = cleaned_data['user_name'].isnull().sum()\n",
    "        if null_names > 0:\n",
    "            print(f\"‚ö†Ô∏è  Encontrados {null_names} nombres de usuario nulos\")\n",
    "            cleaned_data['user_name'] = cleaned_data['user_name'].fillna('Usuario_Desconocido')\n",
    "        \n",
    "        # 2. Corregir valores err√≥neos en total_spent\n",
    "        # Convertir valores no num√©ricos a NaN, luego reemplazar -999 por 0\n",
    "        cleaned_data['total_spent'] = pd.to_numeric(cleaned_data['total_spent'], errors='coerce')\n",
    "        invalid_spent = cleaned_data['total_spent'].isnull().sum()\n",
    "        if invalid_spent > 0:\n",
    "            print(f\"‚ö†Ô∏è  Encontrados {invalid_spent} valores inv√°lidos en total_spent\")\n",
    "            cleaned_data['total_spent'] = cleaned_data['total_spent'].fillna(0)\n",
    "        \n",
    "        cleaned_data['total_spent'] = cleaned_data['total_spent'].replace(-999, 0)\n",
    "        \n",
    "        # 3. Normalizar c√≥digos de pa√≠s\n",
    "        cleaned_data['country_code'] = cleaned_data['country_code'].str.upper()\n",
    "        \n",
    "        # 4. Aplicar regla de negocio: solo usuarios con gasto > 50\n",
    "        initial_count = len(cleaned_data)\n",
    "        cleaned_data = cleaned_data[cleaned_data['total_spent'] > 50]\n",
    "        filtered_count = initial_count - len(cleaned_data)\n",
    "        \n",
    "        print(f\"üìä Registros filtrados (gasto <= 50): {filtered_count}\")\n",
    "        print(f\"üìä Registros finales: {len(cleaned_data)}\")\n",
    "        \n",
    "        print(\"\\n--- 2. Datos Limpios (Reglas Aplicadas) ---\")\n",
    "        print(cleaned_data)\n",
    "        print('')\n",
    "        \n",
    "        logger.info(f\"Transformaci√≥n completada: {len(cleaned_data)} registros v√°lidos\")\n",
    "        return cleaned_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la transformaci√≥n: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0c9ff",
   "metadata": {},
   "source": [
    "### üîç T√©cnicas Avanzadas de Transformaci√≥n\n",
    "\n",
    "Veamos algunas t√©cnicas adicionales de transformaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfe0693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_transformations(df):\n",
    "    \"\"\"\n",
    "    Aplica transformaciones avanzadas a los datos.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame a transformar\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con transformaciones aplicadas\n",
    "    \"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    # 1. Crear columnas derivadas\n",
    "    df_transformed['registration_year'] = pd.to_datetime(df_transformed['registration_date']).dt.year\n",
    "    df_transformed['registration_month'] = pd.to_datetime(df_transformed['registration_date']).dt.month\n",
    "    \n",
    "    # 2. Categorizar gastos\n",
    "    def categorize_spending(amount):\n",
    "        if amount < 100:\n",
    "            return 'Bajo'\n",
    "        elif amount < 200:\n",
    "            return 'Medio'\n",
    "        else:\n",
    "            return 'Alto'\n",
    "    \n",
    "    df_transformed['spending_category'] = df_transformed['total_spent'].apply(categorize_spending)\n",
    "    \n",
    "    # 3. Validar emails (ejemplo b√°sico)\n",
    "    if 'email' in df_transformed.columns:\n",
    "        df_transformed['email_valid'] = df_transformed['email'].str.contains('@', na=False)\n",
    "    \n",
    "    print(\"--- Transformaciones Avanzadas ---\")\n",
    "    print(df_transformed[['user_name', 'total_spent', 'spending_category', 'registration_year']].head())\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "# Esta funci√≥n se puede usar despu√©s de apply_business_rules\n",
    "# df_advanced = advanced_transformations(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b8a6f",
   "metadata": {},
   "source": [
    "## 3. Transform: Normalizaci√≥n del Esquema\n",
    "\n",
    "### üéØ Objetivos de la Normalizaci√≥n\n",
    "- Estandarizar nombres de columnas\n",
    "- Asegurar tipos de datos correctos\n",
    "- Mantener consistencia entre sistemas\n",
    "- Facilitar la integraci√≥n con el destino\n",
    "\n",
    "El objetivo es asegurar que el esquema de los datos (nombres de columnas, tipos de datos) sea consistente.\n",
    "\n",
    "**Ejemplo:** Vamos a:\n",
    "1. Cambiar nombres de columnas a un formato est√°ndar (snake_case)\n",
    "2. Asegurar que `registration_date` sea de tipo fecha\n",
    "3. Reordenar columnas seg√∫n el esquema del destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d150fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_schema(cleaned_data):\n",
    "    \"\"\"\n",
    "    Normaliza el esquema de datos seg√∫n est√°ndares del data warehouse.\n",
    "    \n",
    "    Args:\n",
    "        cleaned_data (pd.DataFrame): Datos limpios a normalizar\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Datos con esquema normalizado\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Iniciando normalizaci√≥n del esquema...\")\n",
    "        structured_data = cleaned_data.copy()\n",
    "        \n",
    "        print(\"--- An√°lisis del Esquema Original ---\")\n",
    "        print(f\"Columnas originales: {list(structured_data.columns)}\")\n",
    "        print(f\"Tipos de datos originales:\")\n",
    "        print(structured_data.dtypes)\n",
    "        print()\n",
    "        \n",
    "        # 1. Normalizar nombres de columnas (snake_case)\n",
    "        column_mapping = {\n",
    "            'ID_USER': 'user_id',\n",
    "            'user_name': 'user_name',\n",
    "            'registration_date': 'registration_date',\n",
    "            'total_spent': 'total_spent',\n",
    "            'country_code': 'country_code',\n",
    "            'email': 'email_address'\n",
    "        }\n",
    "        \n",
    "        # Solo renombrar columnas que existen\n",
    "        existing_columns = {k: v for k, v in column_mapping.items() if k in structured_data.columns}\n",
    "        structured_data = structured_data.rename(columns=existing_columns)\n",
    "        \n",
    "        # 2. Convertir tipos de datos\n",
    "        structured_data['registration_date'] = pd.to_datetime(structured_data['registration_date'])\n",
    "        structured_data['user_id'] = structured_data['user_id'].astype('int64')\n",
    "        structured_data['total_spent'] = structured_data['total_spent'].astype('float64')\n",
    "        \n",
    "        # 3. Agregar metadatos de procesamiento\n",
    "        structured_data['processed_at'] = datetime.now()\n",
    "        structured_data['data_source'] = 'user_database'\n",
    "        \n",
    "        # 4. Reordenar columnas seg√∫n esquema del data warehouse\n",
    "        column_order = ['user_id', 'user_name', 'email_address', 'country_code', \n",
    "                       'registration_date', 'total_spent', 'processed_at', 'data_source']\n",
    "        \n",
    "        # Solo incluir columnas que existen\n",
    "        available_columns = [col for col in column_order if col in structured_data.columns]\n",
    "        structured_data = structured_data[available_columns]\n",
    "        \n",
    "        print(\"--- 3. Datos Estructurados (Esquema Normalizado) ---\")\n",
    "        print(f\"Columnas finales: {list(structured_data.columns)}\")\n",
    "        print(structured_data)\n",
    "        print(\"\\nInformaci√≥n del esquema final:\")\n",
    "        print(structured_data.info())\n",
    "        print('')\n",
    "        \n",
    "        logger.info(f\"Normalizaci√≥n completada: {len(structured_data)} registros estructurados\")\n",
    "        return structured_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la normalizaci√≥n: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b95624f",
   "metadata": {},
   "source": [
    "## 4. Load: Carga de Datos\n",
    "\n",
    "### üéØ Objetivos de la Fase Load\n",
    "- Cargar datos en el sistema de destino\n",
    "- Mantener integridad referencial\n",
    "- Optimizar el rendimiento de carga\n",
    "- Implementar estrategias de carga incremental\n",
    "\n",
    "### üìä Estrategias de Carga\n",
    "- **Full Load**: Carga completa de todos los datos\n",
    "- **Incremental Load**: Solo datos nuevos o modificados\n",
    "- **Upsert**: Insertar nuevos registros, actualizar existentes\n",
    "- **SCD (Slowly Changing Dimensions)**: Manejo de cambios hist√≥ricos\n",
    "\n",
    "La fase final es cargar los datos transformados en el sistema de destino (Data Warehouse, Data Lake, etc.).\n",
    "\n",
    "**Ejemplo:** Simularemos diferentes tipos de carga:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_warehouse(structured_data, load_type='full'):\n",
    "    \"\"\"\n",
    "    Carga datos al data warehouse con diferentes estrategias.\n",
    "    \n",
    "    Args:\n",
    "        structured_data (pd.DataFrame): Datos estructurados a cargar\n",
    "        load_type (str): Tipo de carga ('full', 'incremental', 'upsert')\n",
    "    \n",
    "    Returns:\n",
    "        bool: True si la carga fue exitosa\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Iniciando carga de datos - Tipo: {load_type}\")\n",
    "        \n",
    "        # Validaciones pre-carga\n",
    "        if structured_data.empty:\n",
    "            logger.warning(\"No hay datos para cargar\")\n",
    "            return False\n",
    "        \n",
    "        print(\"--- Validaciones Pre-Carga ---\")\n",
    "        print(f\"Registros a cargar: {len(structured_data)}\")\n",
    "        print(f\"Columnas: {list(structured_data.columns)}\")\n",
    "        \n",
    "        # Verificar duplicados\n",
    "        duplicates = structured_data.duplicated(subset=['user_id']).sum()\n",
    "        if duplicates > 0:\n",
    "            print(f\"‚ö†Ô∏è  Encontrados {duplicates} registros duplicados\")\n",
    "            structured_data = structured_data.drop_duplicates(subset=['user_id'], keep='last')\n",
    "        \n",
    "        # Simular diferentes tipos de carga\n",
    "        if load_type == 'full':\n",
    "            # Carga completa - reemplaza todos los datos\n",
    "            filename = 'data_warehouse_users_full.csv'\n",
    "            structured_data.to_csv(filename, index=False)\n",
    "            print(f\"‚úÖ Carga completa realizada: {filename}\")\n",
    "            \n",
    "        elif load_type == 'incremental':\n",
    "            # Carga incremental - solo nuevos registros\n",
    "            filename = 'data_warehouse_users_incremental.csv'\n",
    "            # En un caso real, verificar√≠amos qu√© registros ya existen\n",
    "            structured_data.to_csv(filename, mode='a', header=False, index=False)\n",
    "            print(f\"‚úÖ Carga incremental realizada: {filename}\")\n",
    "            \n",
    "        elif load_type == 'upsert':\n",
    "            # Upsert - insertar nuevos, actualizar existentes\n",
    "            filename = 'data_warehouse_users_upsert.csv'\n",
    "            structured_data.to_csv(filename, index=False)\n",
    "            print(f\"‚úÖ Upsert realizado: {filename}\")\n",
    "        \n",
    "        print(\"\\n--- 4. Datos Cargados ---\")\n",
    "        print(f\"Registros cargados exitosamente: {len(structured_data)}\")\n",
    "        print(\"\\nPrimeras filas del archivo de destino:\")\n",
    "        \n",
    "        # Mostrar contenido del archivo\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()[:6]  # Mostrar solo las primeras 6 l√≠neas\n",
    "            print(''.join(lines))\n",
    "        \n",
    "        # Estad√≠sticas de carga\n",
    "        print(\"\\n--- Estad√≠sticas de Carga ---\")\n",
    "        print(f\"üìä Total de registros: {len(structured_data)}\")\n",
    "        print(f\"üìä Pa√≠ses √∫nicos: {structured_data['country_code'].nunique()}\")\n",
    "        print(f\"üìä Gasto promedio: ${structured_data['total_spent'].mean():.2f}\")\n",
    "        print(f\"üìä Gasto total: ${structured_data['total_spent'].sum():.2f}\")\n",
    "        \n",
    "        logger.info(f\"Carga completada exitosamente: {len(structured_data)} registros\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la carga: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f0dd3",
   "metadata": {},
   "source": [
    "## Ejecuci√≥n del Pipeline Completo\n",
    "\n",
    "Finalmente, orquestamos todas las funciones en el pipeline principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_pipeline_with_monitoring():\n",
    "    \"\"\"\n",
    "    Pipeline ETL completo con monitoreo y manejo de errores.\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        print(\"üöÄ Iniciando Pipeline ETL\")\n",
    "        print(f\"‚è∞ Hora de inicio: {start_time}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Extract\n",
    "        raw_data = extract_from_database()\n",
    "        \n",
    "        # Transform\n",
    "        cleaned_data = apply_business_rules(raw_data)\n",
    "        \n",
    "        # Normalize\n",
    "        structured_data = normalize_schema(cleaned_data)\n",
    "        \n",
    "        # Load\n",
    "        success = load_to_warehouse(structured_data, load_type='full')\n",
    "        \n",
    "        # M√©tricas finales\n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üìä RESUMEN DEL PIPELINE ETL\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"‚úÖ Estado: {'EXITOSO' if success else 'FALLIDO'}\")\n",
    "        print(f\"‚è±Ô∏è  Duraci√≥n total: {duration.total_seconds():.2f} segundos\")\n",
    "        print(f\"üì• Registros extra√≠dos: {len(raw_data)}\")\n",
    "        print(f\"üîÑ Registros procesados: {len(cleaned_data)}\")\n",
    "        print(f\"üì§ Registros cargados: {len(structured_data)}\")\n",
    "        print(f\"üìâ Tasa de filtrado: {((len(raw_data) - len(structured_data)) / len(raw_data) * 100):.1f}%\")\n",
    "        \n",
    "        return structured_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline fall√≥: {str(e)}\")\n",
    "        print(f\"‚ùå Error en el pipeline: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Ejecutar el pipeline completo\n",
    "result = etl_pipeline_with_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50e4c7",
   "metadata": {},
   "source": [
    "## üîß Ejecuci√≥n Paso a Paso\n",
    "\n",
    "Si quieres ver cada paso por separado, ejecuta las siguientes celdas una por una:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69763808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Extracci√≥n\n",
    "print(\"=== PASO 1: EXTRACCI√ìN ===\")\n",
    "raw_data_step = extract_from_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1633ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Transformaci√≥n\n",
    "print(\"=== PASO 2: TRANSFORMACI√ìN ===\")\n",
    "cleaned_data_step = apply_business_rules(raw_data_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457582c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Normalizaci√≥n\n",
    "print(\"=== PASO 3: NORMALIZACI√ìN ===\")\n",
    "structured_data_step = normalize_schema(cleaned_data_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbcc55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Carga\n",
    "print(\"=== PASO 4: CARGA ===\")\n",
    "success = load_to_warehouse(structured_data_step, load_type='full')\n",
    "print(f\"\\n‚úÖ Pipeline completado exitosamente: {success}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
